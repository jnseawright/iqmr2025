---
title: "Multi-method Designs for Concept-formation, Theory-building, and Measurement"
subtitle: "Multi-Method Research II"
author: "<large>Jaye Seawright</large>"
institute: "<small>IQMR 2025</small>" 
date: "June 27, 2025"
output: 
  xaringan::moon_reader:
    css: xaringan-themer.css
  
---
class: center, middle

```{css, echo=FALSE}
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
```

```{r, load_refs, include=FALSE, cache=FALSE}
# Initializes the bibliography
library(RefManageR)

library(knitr)
library(ggplot2)
library(dplyr)
library(readr)
library(nlme)
library(jtools)
library(hrbrthemes)
library(mice)

BibOptions(check.entries = FALSE,
           bib.style = "authoryear", # Bibliography style
           max.names = 3, # Max author names displayed in bibliography
           sorting = "nyt", #Name, year, title sorting
           cite.style = "authoryear", # citation style
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
#myBib <- ReadBib("assets/myBib.bib", check = FALSE)
# Note: don't forget to clear the knitr cache to account for changes in the
# bibliography.
```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer,MnSymbol)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  text_font_size = "1.6rem"
)
```

---
### What Makes a Good Concept?

In the Sartori tradition, we want to:

-   Choose usages that preserve the meanings of neighboring concepts

-   Respect common and scholarly meanings for the terms in question

-   Capture well-established core defining attributes

-   Make sure that prototypical cases are sorted appropriately


---
### Example: Extremism

There are over 2000 articles in Political Science and Sociology journals
since 1990 that use the concept of extremism ("extremists", etc.).


---
![image](media/extremismR1.png)


---
![image](media/extremismtopics.png)

---
### Example: Extremism

How do people in general think about extremism?

---
![image](media/extremismfg1.png)


---
![image](media/extremismfg2.png)

---
### Example: Extremism

How can we check if the patterns that we see hold up systematically?

---
![image](media/extremismfactoranalysiscitation.png)


---
![image](media/extremismfactoranalysisabstract.png)


---
![image](media/extremismfactoranalysis.png)


---
### Factor Analysis

Assumptions:

-   Meaning

-   Descriptive homogeneity


---
### Factor Analysis

Multi-method design components:

-   Meaning: choose highly prototypical cases, look in-depth

-   Descriptive homogeneity: focus groups or process tracing of coders


---
### Multi-Method Conceptualization and Measurement


---
### The Challenge of Building Theory

Deductive process tracing and the assumption that all alternative
explanations are at hand.


---
### QCA

```{r, echo = TRUE}
library(QCA)
ttLC <- truthTable(LC, "SURV")
ttLC
```

---

```{r, echo = TRUE}
minimize(ttLC)
```

---
### QCA

Unfortunately, Marx and Dusa show that QCA using more than 4-7 independent variables (depending on sample size) is highly likely to generate false positives.

---
### Machine Learning


---
### CART

Optimally predicting $Y$ based on $\mathbb{X}$, without assumptions of
additivity, linearity, etc.


---
### CART

1.  Start with the set of all cases, i.e., the root node.

2.  Search all values of each variable in $\mathbb{X}$ for the binary
    split that maximizes homogeneity of $Y$ for cases on each side of
    the split.

3.  If homogeneity of $Y$ is sufficient or if the remaining sets of
    cases as the new final nodes are too small, stop. Otherwise, repeat
    step 2 for each of the current last-generation nodes.


---
### An Example

![image](media/experimentcart.png)


---
### Simulation Studies

$$Y = X_{1} + X_{2} x_{3} X_{4} + X_{3} X_{5}$$


---
### Simulation Studies

$$Y = X_{1} X_{9} + X_{2} x_{3} X_{4} x_{9} + X_{3} X_{5} X_{10}$$

$X_{9}$ and $X_{10}$ are unobserved.


---
QCA: No Omitted Variables

![image](media/QCAnoomiited.png)


---
CART: No Omitted Variables

![image](media/CARTnoomitted.png)


---
QCA: Omitted Variables

![image](media/QCAmissing5.png)


---
CART: Omitted Variables

![image](media/CARTmissing5.png)

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
library(rpart)
qog_std_ts_jan22 <- read.csv("data/qog_std_ts_jan22.csv")
```

---
```{r, echo = TRUE, out.width="60%", fig.retina = 1, fig.align='center'}
dem.cart <- rpart(vdem_libdem ~ vdem_gender + vdem_corr + wdi_gdpcapcon2010 + wdi_mobile + wdi_gerp + une_surlgpef + wdi_fertility, data=qog_std_ts_jan22, na.action=na.omit)

plot(dem.cart)
text(dem.cart, use.n=TRUE)
```



---
### More current tools than CART for Multi-Method Research

---
### Random Forests

1.  Bootstrap the underlying data.

2.  Run CART, selecting a random subset of datapoints and variables at each
    decision node.

3.  Repeat several times, and find a way to average the results
    together.

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
library(randomForest)
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
dem.rf <- randomForest(vdem_libdem ~ wdi_gendeqr + bci_bci + wdi_gdpcapcon2010 + wdi_mobile + wdi_gerp + une_surlgpef + wdi_fertility, data=qog_std_ts_jan22, na.action=na.omit,
                       localImp=TRUE)

dem.rf
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
library(randomForestExplainer)
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
dem.mindepth <- min_depth_distribution(dem.rf)
plot_min_depth_distribution(dem.mindepth)
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
plot_min_depth_interactions(dem.rf)
```

---
### Hands-On

From [this website](https://jnseawright.github.io/practice-of-multimethod/Chapter-2.html#How_do_scholars_use_the_idea_of_backsliding), work on the exercise, *How Do Scholars Use the Idea of Backsliding?*


---
### Picking Control Variables

Throughout this course, we've often assumed we could correctly identify control variables. Can we, though?

---
### Picking Control Variables

Suppose we know enough to rule out post-treatment variables and colliders, but possibly not enough to distinguish a priori between confounders, irrelevant controls, and instruments. Suppose also that the total number of potential controls is very large. What to do?

---
### LASSO

Estimate a regression, as usual (minimizing the sum of squared errors), but subject to the added penalty term:

$$\lambda \sum_{j=1}^{p}|\beta_{j}|$$

---
### Picking Control Variables

If you are pretty sure you don't have any potential instruments in the data, and that there are a reasonable number of causes of the treatment that are not included in the dataset, then there is a double selection LASSO approach to picking variables.

---
```{r, echo = FALSE, out.width="90%", fig.retina = 1, fig.align='center'}
include_graphics("media/Belloni1.png")
```

---
### Double-selection

1. Run a LASSO regression predicting the outcome based on all the possible confounders.

2. Run a second LASSO regression predicting the treatment based on all the possible confounders.

3. Select all variables that have non-zero coefficients in either regression and use them as controls in your actual causal inference.

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
qogwdi <- qog_std_ts_jan22 %>% filter(year > 1979) %>% select(starts_with("wdi") | starts_with("year") | starts_with("cname"))
qogwdi <- qogwdi[,colSums(is.na(qogwdi))<3000]
qogwdi$vdem_libdem <- qog_std_ts_jan22$vdem_libdem[qog_std_ts_jan22$cname %in% qogwdi$cname & qog_std_ts_jan22$year >1979]
qogwdi.imp <- mice(qogwdi, method = "cart")
```
---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
qogimputed1 <- mice::complete(qogwdi.imp)

summary(lm(wdi_gdpcapgr~vdem_libdem + I(log(wdi_gdpcapcon2010)) + wdi_gerp, data=qogimputed1))
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
library(hdm)
```


---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
demgrow.dualselection <- rlassoEffects(wdi_gdpcapgr ~ . - year - cname - cname_qog - cname_year - wdi_gdpgr - wdi_popgr, data=qogimputed1, I=~vdem_libdem)
demgrow.dualselection
demgrow.dualselection$selection.matrix
```

---
### Double-selection

One problem with double selection is that you will get instruments as control variables if they're in the data. Another problem is that it's inefficient because you aren't using information about the two relationships of interest jointly.

---
```{r, echo = FALSE, out.width="90%", fig.retina = 1, fig.align='center'}
include_graphics("media/Shortreed1.png")
```

---
### Adaptive LASSO

Adaptive LASSO allows the penalty term to change across coefficients:

$$\lambda \sum_{j=1}^{p}|\hat{w}_{j}\beta_{j}|$$

In a standard adaptive LASSO, $\hat{w} = |\tilde{\beta}_{j}|^{-\gamma}$, where $\tilde{\beta}_{j}$ is a typical parametric estimate of the regression slopes and $\gamma$ is a positive penalty parameter.

---
### Outcome-Adaptive LASSO

1. Run some kind of regression, predicting the outcome of interest with the treatment and all the potential control variables.

2. Run an adaptive lasso predicting the treatment based on the control variables. Set the penalty terms based on the coefficient estimates from step 1.

3. Use the control variables selected in step 2 to estimate a propensity score (or a regression or whatever).


---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
library(glmnet)
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
qogimpcomplete <- qogimputed1[,colSums(is.na(qogimputed1))<1]

demgrow.adaptivelasso.part1 <- lm(wdi_gdpcapgr ~ . - year - cname - cname_qog - cname_year - wdi_gdpgr - wdi_popgr, data=qogimpcomplete)
tildebeta <- demgrow.adaptivelasso.part1$coefficients[2:59]
tildebeta[is.na(tildebeta)] <- .0000000000000000000000000000000000000000000000000000001
gammaval <- 0.5

xmat <- qogimpcomplete[,-c(17,20,49,62:66)]

demgrow.adaptivelasso.part2 <- glmnet(y=qogimputed1$wdi_gdpcapgr, x=xmat, penalty.factor=abs(tildebeta)^(-1*gammaval))
```

---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
coef(demgrow.adaptivelasso.part2, 1)
```
---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
coef(demgrow.adaptivelasso.part2, 2)
```
---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
coef(demgrow.adaptivelasso.part2, 5)
```
---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
coef(demgrow.adaptivelasso.part2, 20)
```
---
```{r, echo = TRUE, out.width="90%", fig.retina = 1, fig.align='center'}
coef(demgrow.adaptivelasso.part2, 100)
```

